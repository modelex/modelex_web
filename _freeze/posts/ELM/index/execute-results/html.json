{
  "hash": "553f3ad43a6295ae55d4a5f7556c2d05",
  "result": {
    "markdown": "---\ntitle: \"Extreme Learning Machine (ELM) algorithm \"\nauthor: \"\"\ndate: \"2023-02-14\"\ncategories: [R, code, Extreme Learning Machine, ELM]\n---\n\n\n## R code implementation of the Extreme Learning Machine (ELM) algorithm\n\nThis code defines two functions: **`ELM`** and **`predict`**. The **`ELM`** function takes a data matrix **`X`**, a response vector **`y`**, the number of hidden units **`hidden_size`**, and the activation function **`activation_function`** as input and returns an ELM model consisting of the input weights **`W`**, the input biases **`b`**, the output weights **`beta`**, and the activation function. The function first initializes the input weights and biases randomly, then calculates the output of the hidden layer using the specified activation function, and finally calculates the output weights by solving a linear system. The **`predict`** function takes an ELM model and a data matrix **`X`** as input and returns the predicted output.\n\nIn this example, we generate some example data using the **`runif`** and **`rnorm`** functions, and then apply the **`ELM`** function to the data to get the ELM model. Finally, we use the **`predict`** function to get the predicted output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the ELM function\nELM <- function(X, y, hidden_size, activation_function = \"sigmoid\") {\n  n <- nrow(X)\n  d <- ncol(X)\n  # Initialize the input weights and biases\n  W <- matrix(rnorm(d * hidden_size), ncol = hidden_size)\n  b <- rnorm(hidden_size)\n  # Calculate the hidden layer output\n  if (activation_function == \"sigmoid\") {\n    H <- 1 / (1 + exp(-X %*% W - b))\n  } else if (activation_function == \"relu\") {\n    H <- pmax(0, X %*% W + b)\n  }\n  # Calculate the output weights\n  beta <- solve(H) %*% y\n  # Return the ELM model\n  return(list(W = W, b = b, beta = beta, activation_function = activation_function))\n}\n\n# Define the function to predict the output\npredict <- function(model, X) {\n  if (model$activation_function == \"sigmoid\") {\n    H <- 1 / (1 + exp(-X %*% model$W - model$b))\n  } else if (model$activation_function == \"relu\") {\n    H <- pmax(0, X %*% model$W + model$b)\n  }\n  y_pred <- H %*% model$beta\n  return(y_pred)\n}\n\n# Generate some example data\nset.seed(123)\nX <- matrix(runif(100 * 5), ncol = 5)\ny <- rnorm(100)\n\n# Apply the ELM function to the data\nhidden_size <- 100\nactivation_function <- \"sigmoid\"\nmodel <- ELM(X, y, hidden_size, activation_function)\n\n# Predict the output\ny_pred <- predict(model, X)\n```\n:::\n\n\n### More complex function\n\n\n::: {.cell}\n\n```{.r .cell-code}\n elm <- function(X, y, n_hidden=NULL, active_fun=tanh) {\n  # X: an N observations x p features matrix\n  # y: the target\n  # n_hidden: the number of hidden nodes\n  # active_fun: activation function\n  pp1 = ncol(X) + 1\n  w0 = matrix(rnorm(pp1*n_hidden), pp1, n_hidden)       # random weights\n  h = active_fun(cbind(1, scale(X)) %*% w0)             # compute hidden layer\n  B = MASS::ginv(h) %*% y                               # find weights for hidden layer\n  fit = h %*% B                                         # fitted values\n  list(fit= fit, loss=crossprod(fit - y), B=B, w0=w0)\n}\n\n\n\n# one variable, complex function -------------------------------------------\nlibrary(tidyverse); library(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLoading required package: nlme\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n```\n:::\n\n```{.r .cell-code}\nset.seed(123)\nn = 5000\nx = runif(n)\n# x = rnorm(n)\nmu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))\ny = rnorm(n, mu, .3)\n# qplot(x, y)\nd = data.frame(x,y) \n\nX_ = as.matrix(x, ncol=1)\n\ntest = elm(X_, y, n_hidden=100)\nstr(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ fit : num [1:5000, 1] -1.0239 0.7311 -0.413 0.0806 -0.4112 ...\n $ loss: num [1, 1] 442\n $ B   : num [1:100, 1] 217 -608 1408 -1433 -4575 ...\n $ w0  : num [1:2, 1:100] 0.35 0.814 -0.517 -2.692 -1.097 ...\n```\n:::\n\n```{.r .cell-code}\n# qplot(x, y) + geom_line(aes(y=test$fit), color='#1e90ff')\ncor(test$fit[,1], y)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8862518\n```\n:::\n\n```{.r .cell-code}\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8482127\n```\n:::\n\n```{.r .cell-code}\nd %>% \n  mutate(fit_elm = test$fit,\n         fit_gam = fitted(gam_comparison)) %>% \n  ggplot() + \n  geom_point(aes(x, y), alpha=.1) +\n  geom_line(aes(x, y=fit_elm), color='#1e90ff') + \n  geom_line(aes(x, y=fit_gam), color='darkred')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
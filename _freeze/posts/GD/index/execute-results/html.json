{
  "hash": "006efeb2d548145872a5a301c4ac1307",
  "result": {
    "markdown": "---\ntitle: \"Gradient Descent\"\nauthor: \"\"\ndate: \"2023-02-14\"\ncategories: [R, code, Gradient Descent]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n#' # Data Setup\n#' \n#' Create some basic data for standard regression.\n\nset.seed(8675309)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)  # model matrix\n\n\n\n#' # Gradient Descent Algorithm\n\n\ngd = function(\n  par,\n  X,\n  y,\n  tolerance = 1e-3,\n  maxit     = 1000,\n  stepsize  = 1e-3,\n  adapt     = FALSE,\n  verbose   = TRUE,\n  plotLoss  = TRUE\n  ) {\n  \n  # initialize\n  beta = par; names(beta) = colnames(X)\n  loss = crossprod(X %*% beta - y)\n  tol  = 1\n  iter = 1\n  \n  while(tol > tolerance && iter < maxit){\n    \n    LP   = X %*% beta\n    grad = t(X) %*% (LP - y)\n    betaCurrent = beta - stepsize * grad\n    tol  = max(abs(betaCurrent - beta))\n    beta = betaCurrent\n    loss = append(loss, crossprod(LP - y))\n    iter = iter + 1\n    \n    if (adapt)\n      stepsize = ifelse(\n        loss[iter] < loss[iter - 1],  \n        stepsize * 1.2, \n        stepsize * .8\n      )\n    \n    if (verbose && iter %% 10 == 0)\n      message(paste('Iteration:', iter))\n  }\n  \n  if (plotLoss)\n    plot(loss, type = 'l', bty = 'n')\n  \n  list(\n    par    = beta,\n    loss   = loss,\n    RSE    = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), \n    iter   = iter,\n    fitted = LP\n  )\n}\n\n\n#' ## Run\n#' \n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you'd have to fiddle with the `stepsize`, which could \n#' be assessed via cross-validation, or alternatively one can use an\n#' adaptive approach, a simple one of which is implemented in this function.\n\ngd_result = gd(\n  init,\n  X = X,\n  y = y,\n  tolerance = 1e-8,\n  stepsize  = 1e-4,\n  adapt     = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nIteration: 10\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIteration: 20\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIteration: 30\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIteration: 40\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIteration: 50\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIteration: 60\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIteration: 70\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nstr(gd_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 5\n $ par   : num [1:3, 1] 0.985 0.487 0.218\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ loss  : num [1:70] 2315 2315 2075 1918 1760 ...\n $ RSE   : num [1, 1] 1.03\n $ iter  : num 70\n $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ...\n```\n:::\n\n```{.r .cell-code}\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n\nrbind(\n  gd = round(gd_result$par[, 1], 5),\n  lm = coef(lm(y ~ x1 + x2))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Intercept        x1        x2\ngd 0.9847800 0.4867900 0.2175200\nlm 0.9847803 0.4867896 0.2175169\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
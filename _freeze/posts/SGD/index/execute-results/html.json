{
  "hash": "173c20294f853cf827a9bb961ece4be0",
  "result": {
    "markdown": "---\ntitle: \"Stochastic Gradient Descent\"\nauthor: \"\"\ndate: \"2023-02-14\"\ncategories: [R, code, Stochastic Gradient Descent]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)\n\n\n\n#' # Stochastic Gradient Descent Algorithm\n\n\nsgd = function(\n  par,                                      # parameter estimates\n  X,                                        # model matrix\n  y,                                        # target variable\n  stepsize = 1,                             # the learning rate\n  stepsizeTau = 0,                          # if > 0, a check on the LR at early iterations\n  average = FALSE\n){\n  \n  # initialize\n  beta = par\n  names(beta) = colnames(X)\n  betamat = matrix(0, nrow(X), ncol = length(beta))      # Collect all estimates\n  fits = NA                                              # fitted values\n  s = 0                                                  # adagrad per parameter learning rate adjustment\n  loss = NA                                              # Collect loss at each point\n  \n  for (i in 1:nrow(X)) {\n    Xi   = X[i, , drop = FALSE]\n    yi   = y[i]\n    LP   = Xi %*% beta                                   # matrix operations not necessary, \n    grad = t(Xi) %*% (LP - yi)                           # but makes consistent with the  standard gd R file\n    s    = s + grad^2\n    beta = beta - stepsize * grad/(stepsizeTau + sqrt(s))     # adagrad approach\n    \n    if (average & i > 1) {\n      beta =  beta - 1/i * (betamat[i - 1, ] - beta)          # a variation\n    } \n    \n    betamat[i,] = beta\n    fits[i]     = LP\n    loss[i]     = (LP - yi)^2\n  }\n  \n  LP = X %*% beta\n  lastloss = crossprod(LP - y)\n  \n  list(\n    par    = beta,                                       # final estimates\n    parvec = betamat,                                    # all estimates\n    loss   = loss,                                       # observation level loss\n    RMSE   = sqrt(sum(lastloss)/nrow(X)),\n    fitted = fits\n  )\n}\n\n\n\n#' # Run\n\n\n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you might have to fiddle with the `stepsize`, perhaps\n#' choosing one based on cross-validation with old data.\n\nsgd_result = sgd(\n  init,\n  X = X,\n  y = y,\n  stepsize = .1,\n  stepsizeTau = .5,\n  average = FALSE\n)\n\nstr(sgd_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 5\n $ par   : num [1:3, 1] 1.024 0.537 0.148\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ parvec: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ...\n $ loss  : num [1:1000] 0.67 1.261 1.365 2.043 0.215 ...\n $ RMSE  : num 1.01\n $ fitted: num [1:1000] 0 -0.0236 -0.0446 -0.2828 0.1634 ...\n```\n:::\n\n```{.r .cell-code}\nsgd_result$par\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               [,1]\nIntercept 1.0241049\nx1        0.5368198\nx2        0.1478470\n```\n:::\n\n```{.r .cell-code}\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n#' \n# summary(lm(y ~ x1 + x2))\ncoef1 = coef(lm(y ~ x1 + x2))\n\nrbind(\n  sgd_result = sgd_result$par[, 1],\n  lm = coef1\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Intercept        x1        x2\nsgd_result  1.024105 0.5368198 0.1478470\nlm          1.029957 0.5177020 0.1631026\n```\n:::\n\n```{.r .cell-code}\n#' ## Visualize Estimates\n#' \nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\ngd = data.frame(sgd_result$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>%\n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter',\n               values_to = 'Value') %>%\n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\nggplot(aes(\n  x = Iteration,\n  y = Value,\n  group = Parameter,\n  color = Parameter\n),\ndata = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration == n), size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    size  = 4,\n    data  = filter(gd, Iteration == n)\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#' # Add alternately data shift\n\n#' This data includes a shift of the previous data.\n\nset.seed(1234)\n\nn2   = 1000\nx1.2 = rnorm(n2)\nx2.2 = rnorm(n2)\ny2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2)\nX2 = rbind(X, cbind(1, x1.2, x2.2))\ncoef2 = coef(lm(y2 ~ x1.2 + x2.2))\ny2 = c(y, y2)\n\nn3    = 1000\nx1.3  = rnorm(n3)\nx2.3  = rnorm(n3)\ny3    = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3)\ncoef3 = coef(lm(y3 ~ x1.3 + x2.3))\n\nX3 = rbind(X2, cbind(1, x1.3, x2.3))\ny3 = c(y2, y3)\n\n\n\n#' ## Run\n\n\nsgd_result2 = sgd(\n  init,\n  X = X3,\n  y = y3,\n  stepsize = 1,\n  stepsizeTau = 0,\n  average = FALSE\n)\n\nstr(sgd_result2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 5\n $ par   : num [1:3, 1] 0.821 -0.223 0.211\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ parvec: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ...\n $ loss  : num [1:3000] 0.67 2.31 3.69 30.99 10.58 ...\n $ RMSE  : num 1.57\n $ fitted: num [1:3000] 0 -0.421 -0.797 -4.421 2.952 ...\n```\n:::\n\n```{.r .cell-code}\n#' Compare with `lm` for each data part.\n#' \nsgd_result2$parvec[c(n, n + n2, n + n2 + n3), ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]       [,2]       [,3]\n[1,]  1.0859378  0.5128904  0.1457697\n[2,] -0.9246994  0.2945723 -0.2941759\n[3,]  0.8213521 -0.2229918  0.2112883\n```\n:::\n\n```{.r .cell-code}\nrbind(coef1, coef2, coef3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      (Intercept)         x1         x2\ncoef1   1.0299573  0.5177020  0.1631026\ncoef2  -0.9700427  0.2677020 -0.2868974\ncoef3   1.0453166 -0.2358521  0.2418489\n```\n:::\n\n```{.r .cell-code}\n#' Visualize estimates.\n#' \ngd = data.frame(sgd_result2$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>% \n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter', \n               values_to = 'Value') %>% \n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\n\nggplot(aes(x = Iteration,\n           y = Value,\n           group = Parameter,\n           color = Parameter\n           ),\n       data = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n             size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n    size = 4,\n    show.legend = FALSE\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
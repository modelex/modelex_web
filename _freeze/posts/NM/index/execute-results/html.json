{
  "hash": "e757892a1b02695f748928a4d8f00319",
  "result": {
    "markdown": "---\ntitle: \"Nelder Mead algorithm\"\nauthor: \"\"\ndate: \"2023-02-14\"\ncategories: [R, code, Nelder Mead algorithm]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nnelder_mead = function(\n  f, \n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha    = 1,\n  gamma    = 2,\n  rho      = 0.5,\n  sigma    = 0.5,\n  verbose  = FALSE\n  ) {\n  # init\n  dim = length(x_start)\n  prev_best = f(x_start)\n  no_improv = 0\n  res = list(list(x_start = x_start, prev_best = prev_best))\n  \n  \n  for (i in 1:dim) {\n    x = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res = append(res, list(list(x_start = x, prev_best = score)))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    idx  = sapply(res, `[[`, 2)\n    res  = res[order(idx)]   # ascending order\n    best = res[[1]][[2]]\n    \n    # break after max_iter\n    if (max_iter > 0 & iters >= max_iter) return(res[[1]])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break) return(res[[1]])\n    \n    # centroid\n    x0 = rep(0, dim)\n    for (tup in 1:(length(res)-1)) {\n      for (i in 1:dim) {\n        x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1)\n      }\n    }\n    \n   # reflection\n   xr = x0 + alpha * (x0 - res[[length(res)]][[1]])\n   rscore = f(xr)\n   if (res[[1]][[2]] <= rscore & \n       rscore < res[[length(res)-1]][[2]]) {\n     res[[length(res)]] = list(xr, rscore)\n     next\n   }\n     \n   # expansion\n   if (rscore < res[[1]][[2]]) {\n     # xe = x0 + gamma*(x0 - res[[length(res)]][[1]])   # issue with this\n     xe = x0 + gamma * (xr - x0)   \n     escore = f(xe)\n     if (escore < rscore) {\n       res[[length(res)]] = list(xe, escore)\n       next\n     } else {\n       res[[length(res)]] = list(xr, rscore)\n       next\n     }\n   }\n   \n   # contraction\n   # xc = x0 + rho*(x0 - res[[length(res)]][[1]])  # issue with wiki consistency for rho values (and optim)\n   xc = x0 + rho * (res[[length(res)]][[1]] - x0)\n   cscore = f(xc)\n   if (cscore < res[[length(res)]][[2]]) {\n     res[[length(res)]] = list(xc, cscore)\n     next\n   }\n   \n   # reduction\n   x1   = res[[1]][[1]]\n   nres = list()\n   for (tup in res) {\n     redx  = x1 + sigma * (tup[[1]] - x1)\n     score = f(redx)\n     nres  = append(nres, list(list(redx, score)))\n   }\n   \n   res = nres\n  }\n}\n\n\n\n\n#' ## Example\n#' The function to minimize.\n#' \nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] -1.570797e+00 -2.235577e-07  1.637460e-14\n\n[[2]]\n[1] -1\n```\n:::\n\n```{.r .cell-code}\n#' Compare to `optimx`.  You may see warnings.\noptimx::optimx(\n  par = c(0, 0, 0),\n  fn = f,\n  method = \"Nelder-Mead\",\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta = 0.5,\n    maxit = 1000,\n    reltol = 1e-12\n  )\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(logpar): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(logpar): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n```\n:::\n\n```{.r .cell-code}\n#' ## A Regression Model\n#' \n#' I find a regression model to be more applicable/intuitive for my needs, so\n#' provide an example for that case.\n#' \n#' \n#' ### Data setup\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n\n#' Least squares loss function.\n\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n# lm estimates\nlm.fit(X, y)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x1          x2          x3          x4          x5          x6 \n-0.96214657  0.59432481  0.04864576  0.27573466  0.97525840 -0.07470287 \n```\n:::\n\n```{.r .cell-code}\nnm_result = nelder_mead(\n  f, \n  runif(ncol(X)), \n  max_iter = 2000,\n  no_improve_thr = 1e-12,\n  verbose = FALSE\n)\n\n#' ### Comparison\n#' Compare to `optimx`.\n\nopt_out = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,  # model function\n  method  = 'Nelder-Mead',\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta  = 0.5,\n    #rho\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)\n\nrbind(\n  nm_func = unlist(nm_result),\n  nm_optimx = opt_out[1:7]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  p1       p2         p3        p4        p5          p6\nnm_func   -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389\nnm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054\n             value\nnm_func   501.3155\nnm_optimx 501.3155\n```\n:::\n\n```{.r .cell-code}\n#' # Second version\n#' \n#' This is a more natural R approach in my opinion.\n\nnelder_mead2 = function(\n  f,\n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha = 1,\n  gamma = 2,\n  rho   = 0.5,\n  sigma = 0.5,\n  verbose = FALSE\n) {\n  \n  # init\n  npar = length(x_start)\n  nc = npar + 1\n  prev_best = f(x_start)\n  no_improv = 0\n  res = matrix(c(x_start, prev_best), ncol = nc)\n  colnames(res) = c(paste('par', 1:npar, sep = '_'), 'score')\n  \n  for (i in 1:npar) {\n    x     = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res   = rbind(res, c(x, score))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    res  = res[order(res[, nc]), ]   # ascending order\n    best = res[1, nc]\n    \n    # break after max_iter\n    if (max_iter & iters >= max_iter) return(res[1, ])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break)\n      return(res[1, ])\n    \n    nr = nrow(res)\n    \n    # centroid: more efficient than previous double loop\n    x0 = colMeans(res[(1:npar), -nc])\n    \n    # reflection\n    xr = x0 + alpha * (x0 - res[nr, -nc])\n    \n    rscore = f(xr)\n    \n    if (res[1, 'score'] <= rscore & rscore < res[npar, 'score']) {\n      res[nr,] = c(xr, rscore)\n      next\n    }\n    \n    # expansion\n    if (rscore < res[1, 'score']) {\n      xe = x0 + gamma * (xr - x0)\n      escore = f(xe)\n      if (escore < rscore) {\n        res[nr, ] = c(xe, escore)\n        next\n      } else {\n        res[nr, ] = c(xr, rscore)\n        next\n      }\n    }\n    \n    # contraction\n    xc = x0 + rho * (res[nr, -nc] - x0)\n    \n    cscore = f(xc)\n    \n    if (cscore < res[nr, 'score']) {\n      res[nr,] = c(xc, cscore)\n      next\n    }\n    \n    # reduction\n    x1 = res[1, -nc]\n    \n    nres = res\n    \n    for (i in 1:nr) {\n      redx  = x1 + sigma * (res[i, -nc] - x1)\n      score = f(redx)\n      nres[i, ] = c(redx, score)\n    }\n    \n    res = nres\n  }\n}\n\n\n#' ## Example function\n\nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead2(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        par_1         par_2         par_3         score \n-1.570797e+00 -2.235577e-07  1.599631e-14 -1.000000e+00 \n```\n:::\n\n```{.r .cell-code}\noptimx::optimx(\n  par = c(0, 0, 0), \n  fn = f, \n  method   = \"Nelder-Mead\",\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 1000,\n    reltol = 1e-12\n  )\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(logpar): no non-missing arguments to max; returning -Inf\n\nWarning in max(logpar): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n```\n:::\n\n```{.r .cell-code}\n#' ## A Regression Model\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n# least squares loss\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n\nlm_par = lm.fit(X, y)$coef\n\nnm_par = nelder_mead2(\n  f, \n  runif(ncol(X)),\n  max_iter = 2000,\n  no_improve_thr = 1e-12\n)\n\n\n#' ## Comparison\n#' Compare to `optimx`.\n\nopt_par = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,\n  method   = 'Nelder-Mead',\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)[1:(npreds + 1)]\n\nrbind(\n  lm = lm_par,\n  nm = nm_par,\n  optimx = opt_par,\n  truth  = beta\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in rbind(deparse.level, ...): number of columns of result, 6, is not a\nmultiple of vector length 7 of arg 2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n               p1        p2         p3        p4        p5          p6\nlm     -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287\nnm     -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389\noptimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054\ntruth  -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
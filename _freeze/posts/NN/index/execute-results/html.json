{
  "hash": "53eef84c098846b7012fd100d7abfc8b",
  "result": {
    "markdown": "---\ntitle: \"Neural Network\"\nauthor: \"\"\ndate: \"2023-02-14\"\ncategories: [R, code, Neural Network]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nX = matrix( \n  c(0, 0, 1, \n    0, 1, 1, \n    1, 0, 1, \n    1, 1, 1),\n  nrow  = 4,\n  ncol  = 3,\n  byrow = TRUE\n)\n\n# output dataset            \ny = c(0, 0, 1, 1)\n\n# seed random numbers to make calculation\n# deterministic (just a good practice)\nset.seed(1)\n\n# initialize weights randomly with mean 0\nsynapse_0 = matrix(runif(3, min = -1, max = 1), 3, 1)\n\n# sigmoid function\nnonlin <- function(x, deriv = FALSE) {\n  if (deriv)\n    x * (1 - x)\n  else\n    plogis(x)\n}\n\n\nnn_1 <- function(X, y, synapse_0, maxiter = 10000) {\n  \n  for (iter in 1:maxiter) {\n  \n      # forward propagation\n      layer_0 = X\n      layer_1 = nonlin(layer_0 %*% synapse_0)\n  \n      # how much did we miss?\n      layer_1_error = y - layer_1\n  \n      # multiply how much we missed by the \n      # slope of the sigmoid at the values in layer_1\n      l1_delta = layer_1_error * nonlin(layer_1, deriv = TRUE)\n  \n      # update weights\n      synapse_0 = synapse_0 + crossprod(layer_0, l1_delta)\n  }\n  \n  list(layer_1 = layer_1, layer_1_error = layer_1_error, synapse_0 = synapse_0)\n}\n\nfit_nn = nn_1(X, y, synapse_0)\n\nmessage(\"Output After Training: \\n\", \n        paste0(capture.output(cbind(fit_nn$layer_1, y)), collapse = '\\n'))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nOutput After Training: \n                 y\n[1,] 0.009670417 0\n[2,] 0.007864211 0\n[3,] 0.993590571 1\n[4,] 0.992115835 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX = matrix(\n  c(0, 0, 1,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 1),\n  nrow = 4,\n  ncol = 3,\n  byrow = TRUE\n)\n\ny = matrix(as.integer(xor(X[,1], X[,2])), ncol = 1)  # make the relationship explicit\n\nset.seed(1)\n\n# or do randomly in same fashion\nsynapse_0 = matrix(runif(12, -1, 1), 3, 4)\nsynapse_1 = matrix(runif(12, -1, 1), 4, 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in matrix(runif(12, -1, 1), 4, 1): data length differs from size of\nmatrix: [12 != 4 x 1]\n```\n:::\n\n```{.r .cell-code}\n# synapse_0\n# synapse_1\n\nnn_2 <- function(\n  X,\n  y,\n  synapse_0_start,\n  synapse_1_start,\n  maxiter = 30000,\n  verbose = TRUE\n) {\n    \n  synapse_0 = synapse_0_start\n  synapse_1 = synapse_1_start\n  \n  for (j in 1:maxiter) {\n    layer_1 = plogis(X  %*% synapse_0)              # 4 x 4\n    layer_2 = plogis(layer_1 %*% synapse_1)         # 4 x 1\n    \n    # how much did we miss the target value?\n    layer_2_error = y - layer_2\n    \n    if (verbose && (j %% 10000) == 0) {\n      message(glue::glue(\"Error: {mean(abs(layer_2_error))}\"))\n    }\n  \n    # in what direction is the target value?\n    # were we really sure? if so, don't change too much.\n    layer_2_delta = (y - layer_2) * (layer_2 * (1 - layer_2))\n    \n    # how much did each l1 value contribute to the l2 error (according to the weights)?\n    layer_1_error = layer_2_delta %*% t(synapse_1)\n    \n    # in what direction is the target l1?\n    # were we really sure? if so, don't change too much.  \n    layer_1_delta = tcrossprod(layer_2_delta, synapse_1) * (layer_1 * (1 - layer_1))\n    \n    # update\n    synapse_1 = synapse_1 + crossprod(layer_1, layer_2_delta)\n    synapse_0 = synapse_0 + crossprod(X, layer_1_delta)\n  }\n  \n  list(\n    layer_1_error = layer_1_error,\n    layer_2_error = layer_2_error,\n    synapse_0 = synapse_0,\n    synapse_1 = synapse_1,\n    layer_1 = layer_1,\n    layer_2 = layer_2\n  )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_nn = nn_2(\n  X,\n  y,\n  synapse_0_start = synapse_0,\n  synapse_1_start = synapse_1,\n  maxiter = 30000\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nError: 0.0105538166393651\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nError: 0.00729252475321202\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nError: 0.0058973637409426\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue('Final error: {round(mean(abs(fit_nn$layer_2_error)), 5)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal error: 0.0059\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nround(fit_nn$layer_1, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]  [,2]  [,3]  [,4]\n[1,] 0.259 0.889 0.364 0.445\n[2,] 0.000 0.037 0.978 0.030\n[3,] 0.946 1.000 0.984 0.020\n[4,] 0.016 0.984 1.000 0.001\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nround(cbind(fit_nn$layer_2, y), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1] [,2]\n[1,] 0.002    0\n[2,] 0.993    1\n[3,] 0.994    1\n[4,] 0.008    0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nround(fit_nn$synapse_0, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       [,1]   [,2]   [,3]   [,4]\n[1,]  3.915  7.364  4.705 -3.669\n[2,] -6.970 -5.351  4.337 -3.242\n[3,] -1.050  2.079 -0.559 -0.221\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nround(fit_nn$synapse_1, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        [,1]\n[1,]  10.988\n[2,] -10.733\n[3,]   5.576\n[4,]  -2.987\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
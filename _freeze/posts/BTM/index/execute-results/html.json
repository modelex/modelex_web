{
  "hash": "56cbd441dade5ec3258426933c1fee27",
  "result": {
    "markdown": "---\ntitle: \"Decision tree algorithm \"\nauthor: \"\"\ndate: \"2023-02-14\"\ncategories: [R, code, Decision tree]\n---\n\n\n## R code implementation of the decision tree algorithm\n\nThis code defines two functions: **`decision_tree`** and **`predict`**. The **`decision_tree`** function takes a data matrix **`x`** and a response vector **`y`** as input and returns the root node of the decision tree. The function first checks if all observations have the same class, and if so, returns a leaf node with that class. Otherwise, it searches for the best split by iterating over all features and all possible split points, and selects the split that minimizes the weighted average of the error rates in the two resulting nodes. The function then creates two child nodes for the left and right sub-trees and recursively applies the same algorithm to each sub-tree. The **`predict`** function takes a decision tree node \\`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the decision tree function\ndecision_tree <- function(x, y) {\n  # Create a new tree node\n  node <- list()\n  node$split_variable <- NULL\n  node$split_value <- NULL\n  node$left_child <- NULL\n  node$right_child <- NULL\n  node$class <- NULL\n  # Check if all observations have the same class\n  if (length(unique(y)) == 1) {\n    node$class <- unique(y)\n    return(node)\n  }\n  # Find the best split\n  best_split <- list()\n  best_split$error_rate <- Inf\n  for (j in 1:ncol(x)) {\n    for (value in unique(x[, j])) {\n      left_idx <- x[, j] < value\n      right_idx <- x[, j] >= value\n      if (sum(left_idx) > 0 && sum(right_idx) > 0) {\n        left_y <- y[left_idx]\n        right_y <- y[right_idx]\n        left_error <- sum(left_y != mode(left_y)) / length(left_y)\n        right_error <- sum(right_y != mode(right_y)) / length(right_y)\n        error_rate <- (sum(left_idx) / nrow(x)) * left_error + (sum(right_idx) / nrow(x)) * right_error\n        if (error_rate < best_split$error_rate) {\n          best_split$error_rate <- error_rate\n          best_split$split_variable <- j\n          best_split$split_value <- value\n        }\n      }\n    }\n  }\n  # Create the left and right child nodes and recurse\n  node$split_variable <- best_split$split_variable\n  node$split_value <- best_split$split_value\n  left_idx <- x[, node$split_variable] < node$split_value\n  right_idx <- x[, node$split_variable] >= node$split_value\n  node$left_child <- decision_tree(x[left_idx, ], y[left_idx])\n  node$right_child <- decision_tree(x[right_idx, ], y[right_idx])\n  return(node)\n}\n\n# Define the function to predict the output\npredict <- function(node, x) {\n  if (!is.null(node$class)) {\n    return(node$class)\n  }\n  if (x[node$split_variable] < node$split_value) {\n    return(predict(node$left_child, x))\n  } else {\n    return(predict(node$right_child, x))\n  }\n}\n\n# Generate some example data\nset.seed(123)\nx <- matrix(runif(100 * 2), ncol = 2)\ny <- as.factor(ifelse(x[, 1] + x[, 2] > 1, \"A\", \"B\"))\n\n# Build the decision tree model\ntree <- decision_tree(x, y)\n\n# Predict the output\ny_pred <- sapply(1:nrow(x), function(i) predict(tree, x[i,]))\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
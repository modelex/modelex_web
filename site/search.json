[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MODELEX.IO",
    "section": "",
    "text": "Logistic regression\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nRidge Regression\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nRidge Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nDecision tree algorithm\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nDecision tree\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nk-Nearest Neighbors (KNN)\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nk-Nearest Neighbors\n\n\nKNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nNeural Network\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nNeural Network\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nExtreme Learning Machine (ELM) algorithm\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nExtreme Learning Machine\n\n\nELM\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nStochastic Gradient Descent\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nStochastic Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nNelder Mead algorithm\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nNelder Mead algorithm\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nCubic Spline Model\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nCubic Spline ModeL\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nGradient Descent\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nGradient Descent\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nLasso Regression\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nLasso Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/LR/index.html",
    "href": "posts/LR/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "This code defines two functions: regression and predict. The regression function takes two vectors x and y as input and returns the estimated intercept b0 and slope b1 of the simple linear regression model. The predict function takes the estimated parameters b0 and b1 and an input vector x and returns the predicted output.\nIn this example, we generate some example data using the runif and rnorm functions and then use the regression function to estimate the parameters of the simple linear regression model. Finally, we use the predict function to get the predicted output.\n\n# Define the function to estimate the parameters\nregression <- function(x, y) {\n  n <- length(x)\n  x_mean <- mean(x)\n  y_mean <- mean(y)\n  numerator <- sum((x - x_mean) * (y - y_mean))\n  denominator <- sum((x - x_mean)^2)\n  b1 <- numerator / denominator\n  b0 <- y_mean - b1 * x_mean\n  return(c(b0, b1))\n}\n\n# Define the function to predict the output\npredict <- function(b0, b1, x) {\n  return(b0 + b1 * x)\n}\n\n# Generate some example data\nset.seed(123)\nx <- runif(100)\ny <- 2 + 3 * x + rnorm(100, 0, 0.2)\n\n# Estimate the parameters\nparams <- regression(x, y)\nb0 <- params[1]\nb1 <- params[2]\n\n# Predict the output\ny_hat <- predict(b0, b1, x)"
  },
  {
    "objectID": "posts/LR/index.html#define-the-function-to-estimate-the-parameters",
    "href": "posts/LR/index.html#define-the-function-to-estimate-the-parameters",
    "title": "Linear Regression",
    "section": "Define the function to estimate the parameters",
    "text": "Define the function to estimate the parameters\n\n# Define the function to estimate the parameters\nregression <- function(x, y) {\n  n <- length(x)\n  x_mean <- mean(x)\n  y_mean <- mean(y)\n  numerator <- sum((x - x_mean) * (y - y_mean))\n  denominator <- sum((x - x_mean)^2)\n  b1 <- numerator / denominator\n  b0 <- y_mean - b1 * x_mean\n  return(c(b0, b1))\n}\n\n# Define the function to predict the output\npredict <- function(b0, b1, x) {\n  return(b0 + b1 * x)\n}\n\n# Generate some example data\nset.seed(123)\nx <- runif(100)\ny <- 2 + 3 * x + rnorm(100, 0, 0.2)\n\n# Estimate the parameters\nparams <- regression(x, y)\nb0 <- params[1]\nb1 <- params[2]\n\n# Predict the output\ny_hat <- predict(b0, b1, x)\n\nThis code defines three functions: log_likelihood, optimize_log_likelihood, and predict. The log_likelihood function takes three vectors beta, x, and y as input and returns the log-likelihood of the simple linear regression model. The optimize_log_likelihood function takes an initial guess for the parameters beta and two vectors x and y as input and returns the maximum likelihood estimates for the parameters. The predict function takes the estimated parameters beta_hat and an input vector x and returns the predicted output.\nIn this example, we generate some example data using the runif and rnorm functions, and then use the optimize_log_likelihood function to estimate the parameters of the simple linear regression model using the maximum likelihood method. Finally, we use the predict function to get the predicted output.\n\n# Define the log-likelihood function\nlog_likelihood <- function(beta, x, y) {\n  n <- length(x)\n  mu <- beta[1] + beta[2] * x\n  log_likelihood <- sum(-0.5 * log(2 * pi) - 0.5 * log(var(y)) - 0.5 * ((y - mu)^2)/var(y))\n  return(-log_likelihood)\n}\n\n# Define the function to optimize the log-likelihood function\noptimize_log_likelihood <- function(beta, x, y) {\n  result <- optim(beta, log_likelihood, x = x, y = y, method = \"BFGS\")\n  beta_hat <- result$par\n  return(beta_hat)\n}\n\n# Define the function to predict the output\npredict <- function(beta_hat, x) {\n  return(beta_hat[1] + beta_hat[2] * x)\n}\n\n# Generate some example data\nset.seed(123)\nx <- runif(100)\ny <- 2 + 3 * x + rnorm(100, 0, 0.2)\n\n# Estimate the parameters\nbeta_init <- c(0, 0)\nbeta_hat <- optimize_log_likelihood(beta_init, x, y)\n\n# Predict the output\ny_hat <- predict(beta_hat, x)"
  },
  {
    "objectID": "posts/LR/index.html#r-code-to-estimate-a-simple-linear-regression-model-using-the-maximum-likelihood-method",
    "href": "posts/LR/index.html#r-code-to-estimate-a-simple-linear-regression-model-using-the-maximum-likelihood-method",
    "title": "Linear Regression",
    "section": "R code to estimate a simple linear regression model using the maximum likelihood method",
    "text": "R code to estimate a simple linear regression model using the maximum likelihood method\nThis code defines three functions: log_likelihood, optimize_log_likelihood, and predict. The log_likelihood function takes three vectors beta, x, and y as input and returns the log-likelihood of the simple linear regression model. The optimize_log_likelihood function takes an initial guess for the parameters beta and two vectors x and y as input and returns the maximum likelihood estimates for the parameters. The predict function takes the estimated parameters beta_hat and an input vector x and returns the predicted output.\nIn this example, we generate some example data using the runif and rnorm functions, and then use the optimize_log_likelihood function to estimate the parameters of the simple linear regression model using the maximum likelihood method. Finally, we use the predict function to get the predicted output.\n\n# Define the log-likelihood function\nlog_likelihood <- function(beta, x, y) {\n  n <- length(x)\n  mu <- beta[1] + beta[2] * x\n  log_likelihood <- sum(-0.5 * log(2 * pi) - 0.5 * log(var(y)) - 0.5 * ((y - mu)^2)/var(y))\n  return(-log_likelihood)\n}\n\n# Define the function to optimize the log-likelihood function\noptimize_log_likelihood <- function(beta, x, y) {\n  result <- optim(beta, log_likelihood, x = x, y = y, method = \"BFGS\")\n  beta_hat <- result$par\n  return(beta_hat)\n}\n\n# Define the function to predict the output\npredict <- function(beta_hat, x) {\n  return(beta_hat[1] + beta_hat[2] * x)\n}\n\n# Generate some example data\nset.seed(123)\nx <- runif(100)\ny <- 2 + 3 * x + rnorm(100, 0, 0.2)\n\n# Estimate the parameters\nbeta_init <- c(0, 0)\nbeta_hat <- optimize_log_likelihood(beta_init, x, y)\n\n# Predict the output\ny_hat <- predict(beta_hat, x)"
  },
  {
    "objectID": "posts/Logistic/index.html",
    "href": "posts/Logistic/index.html",
    "title": "Logistic regression",
    "section": "",
    "text": "This code defines three functions: log_likelihood, optimize_log_likelihood, and predict. The log_likelihood function takes three vectors beta, x, and y as input and returns the log-likelihood of the logistic regression model. The optimize_log_likelihood function takes an initial guess for the parameters beta and two vectors x and y as input and returns the maximum likelihood estimates for the parameters. The predict function takes the estimated parameters beta_hat and an input vector x and returns the predicted probabilities of the outcome.\nIn this example, we generate some example data using the runif and rbinom functions, and then use the optimize_log_likelihood function to estimate the parameters of the logistic regression model using the maximum likelihood method. Finally, we use the predict function to get the predicted probabilities of the outcome.\n\n# Define the log-likelihood function\nlog_likelihood <- function(beta, x, y) {\n  n <- length(x)\n  p <- 1 / (1 + exp(-beta[1] - beta[2] * x))\n  log_likelihood <- sum(y * log(p) + (1 - y) * log(1 - p))\n  return(-log_likelihood)\n}\n\n# Define the function to optimize the log-likelihood function\noptimize_log_likelihood <- function(beta, x, y) {\n  result <- optim(beta, log_likelihood, x = x, y = y, method = \"BFGS\")\n  beta_hat <- result$par\n  return(beta_hat)\n}\n\n# Define the function to predict the output\npredict <- function(beta_hat, x) {\n  p <- 1 / (1 + exp(-beta_hat[1] - beta_hat[2] * x))\n  return(p)\n}\n\n# Generate some example data\nset.seed(123)\nx <- runif(100)\ny <- rbinom(100, size = 1, prob = 1 / (1 + exp(-2 - 3 * x)))\n\n# Estimate the parameters\nbeta_init <- c(0, 0)\nbeta_hat <- optimize_log_likelihood(beta_init, x, y)\n\n# Predict the output\np_hat <- predict(beta_hat, x)"
  },
  {
    "objectID": "posts/Logistic/index.html#r-code-to-estimate-a-standard-logistic-regression-model-via-maximum-likelihood",
    "href": "posts/Logistic/index.html#r-code-to-estimate-a-standard-logistic-regression-model-via-maximum-likelihood",
    "title": "Logistic regression",
    "section": "R code to estimate a standard logistic regression model via maximum likelihood",
    "text": "R code to estimate a standard logistic regression model via maximum likelihood\nThis code defines three functions: log_likelihood, optimize_log_likelihood, and predict. The log_likelihood function takes three matrices beta, X, and y as input and returns the log-likelihood of the logistic regression model. The optimize_log_likelihood function takes an initial guess for the parameters beta and two matrices X and y as input and returns the maximum likelihood estimates for the parameters. The predict function takes the estimated parameters beta_hat and a design matrix X and returns the predicted probabilities of the outcome.\nIn this example, we generate some example data using the rnorm and rbinom functions, and then use the optimize_log_likelihood function to estimate the parameters of the logistic regression model using the maximum likelihood method. Finally, we use the predict function to get the predicted probabilities of the outcome.\n\n# Define the log-likelihood function\nlog_likelihood <- function(beta, X, y) {\n  n <- length(y)\n  p <- 1 / (1 + exp(-X %*% beta))\n  log_likelihood <- sum(y * log(p) + (1 - y) * log(1 - p))\n  return(-log_likelihood)\n}\n\n# Define the function to optimize the log-likelihood function\noptimize_log_likelihood <- function(beta, X, y) {\n  result <- optim(beta, log_likelihood, X = X, y = y, method = \"BFGS\")\n  beta_hat <- result$par\n  return(beta_hat)\n}\n\n# Define the function to predict the output\npredict <- function(beta_hat, X) {\n  p <- 1 / (1 + exp(-X %*% beta_hat))\n  return(p)\n}\n\n# Generate some example data\nset.seed(123)\nn <- 100\np <- 5\nX <- matrix(rnorm(n * p), nrow = n)\ncolnames(X) <- paste0(\"X\", 1:p)\nX <- cbind(1, X)\nbeta <- rnorm(p + 1)\ny <- rbinom(n, size = 1, prob = 1 / (1 + exp(-X %*% beta)))\n\n# Estimate the parameters\nbeta_init <- rep(0, p + 1)\nbeta_hat <- optimize_log_likelihood(beta_init, X, y)\n\n# Predict the output\np_hat <- predict(beta_hat, X)"
  },
  {
    "objectID": "posts/KNN/index.html",
    "href": "posts/KNN/index.html",
    "title": "k-Nearest Neighbors (KNN)",
    "section": "",
    "text": "This code defines a function KNN that takes a training dataset, a test dataset, the corresponding class labels for the training data, and the number of nearest neighbors k as input and returns the class prediction for each test observation. The function first calculates the Euclidean distance between each test observation and each training observation, then selects the k nearest neighbors, and finally predicts the class by majority vote among the nearest neighbors.\nIn this example, we generate some example data using the rnorm function, and then apply the KNN function to each test observation to get the predicted class. The number of nearest neighbors k is set to 5.\n\n\n\n# Define the KNN function\nKNN <- function(train, test, y_train, k = 1) {\n  distances <- as.matrix(dist(rbind(test, train), method = \"euclidean\"))[1, -1]\n  nearest_neighbors <- sort(distances, index.return = TRUE)$ix[1:k]\n  classes <- y_train[nearest_neighbors]\n  class_prediction <- names(sort(table(classes), decreasing = TRUE))[1]\n  return(class_prediction)\n}\n\n# Generate some example data\nset.seed(123)\ntrain <- matrix(rnorm(40 * 2), ncol = 2)\ncolnames(train) <- c(\"x\", \"y\")\ntrain_classes <- c(\"A\", \"B\")[1 + (train[, 1] > 0)]\n\ntest <- matrix(rnorm(10 * 2), ncol = 2)\ncolnames(test) <- c(\"x\", \"y\")\n\n# Apply the KNN function to each test observation\nk <- 5\ny_pred <- sapply(1:nrow(test), function(i) KNN(train, test[i, ], train_classes, k))"
  },
  {
    "objectID": "posts/BTM/index.html",
    "href": "posts/BTM/index.html",
    "title": "Decision tree algorithm",
    "section": "",
    "text": "This code defines two functions: decision_tree and predict. The decision_tree function takes a data matrix x and a response vector y as input and returns the root node of the decision tree. The function first checks if all observations have the same class, and if so, returns a leaf node with that class. Otherwise, it searches for the best split by iterating over all features and all possible split points, and selects the split that minimizes the weighted average of the error rates in the two resulting nodes. The function then creates two child nodes for the left and right sub-trees and recursively applies the same algorithm to each sub-tree. The predict function takes a decision tree node `\n\n# Define the decision tree function\ndecision_tree <- function(x, y) {\n  # Create a new tree node\n  node <- list()\n  node$split_variable <- NULL\n  node$split_value <- NULL\n  node$left_child <- NULL\n  node$right_child <- NULL\n  node$class <- NULL\n  # Check if all observations have the same class\n  if (length(unique(y)) == 1) {\n    node$class <- unique(y)\n    return(node)\n  }\n  # Find the best split\n  best_split <- list()\n  best_split$error_rate <- Inf\n  for (j in 1:ncol(x)) {\n    for (value in unique(x[, j])) {\n      left_idx <- x[, j] < value\n      right_idx <- x[, j] >= value\n      if (sum(left_idx) > 0 && sum(right_idx) > 0) {\n        left_y <- y[left_idx]\n        right_y <- y[right_idx]\n        left_error <- sum(left_y != mode(left_y)) / length(left_y)\n        right_error <- sum(right_y != mode(right_y)) / length(right_y)\n        error_rate <- (sum(left_idx) / nrow(x)) * left_error + (sum(right_idx) / nrow(x)) * right_error\n        if (error_rate < best_split$error_rate) {\n          best_split$error_rate <- error_rate\n          best_split$split_variable <- j\n          best_split$split_value <- value\n        }\n      }\n    }\n  }\n  # Create the left and right child nodes and recurse\n  node$split_variable <- best_split$split_variable\n  node$split_value <- best_split$split_value\n  left_idx <- x[, node$split_variable] < node$split_value\n  right_idx <- x[, node$split_variable] >= node$split_value\n  node$left_child <- decision_tree(x[left_idx, ], y[left_idx])\n  node$right_child <- decision_tree(x[right_idx, ], y[right_idx])\n  return(node)\n}\n\n# Define the function to predict the output\npredict <- function(node, x) {\n  if (!is.null(node$class)) {\n    return(node$class)\n  }\n  if (x[node$split_variable] < node$split_value) {\n    return(predict(node$left_child, x))\n  } else {\n    return(predict(node$right_child, x))\n  }\n}\n\n# Generate some example data\nset.seed(123)\nx <- matrix(runif(100 * 2), ncol = 2)\ny <- as.factor(ifelse(x[, 1] + x[, 2] > 1, \"A\", \"B\"))\n\n# Build the decision tree model\ntree <- decision_tree(x, y)\n\n# Predict the output\ny_pred <- sapply(1:nrow(x), function(i) predict(tree, x[i,]))"
  },
  {
    "objectID": "posts/ELM/index.html",
    "href": "posts/ELM/index.html",
    "title": "Extreme Learning Machine (ELM) algorithm",
    "section": "",
    "text": "This code defines two functions: ELM and predict. The ELM function takes a data matrix X, a response vector y, the number of hidden units hidden_size, and the activation function activation_function as input and returns an ELM model consisting of the input weights W, the input biases b, the output weights beta, and the activation function. The function first initializes the input weights and biases randomly, then calculates the output of the hidden layer using the specified activation function, and finally calculates the output weights by solving a linear system. The predict function takes an ELM model and a data matrix X as input and returns the predicted output.\nIn this example, we generate some example data using the runif and rnorm functions, and then apply the ELM function to the data to get the ELM model. Finally, we use the predict function to get the predicted output.\n\n# Define the ELM function\nELM <- function(X, y, hidden_size, activation_function = \"sigmoid\") {\n  n <- nrow(X)\n  d <- ncol(X)\n  # Initialize the input weights and biases\n  W <- matrix(rnorm(d * hidden_size), ncol = hidden_size)\n  b <- rnorm(hidden_size)\n  # Calculate the hidden layer output\n  if (activation_function == \"sigmoid\") {\n    H <- 1 / (1 + exp(-X %*% W - b))\n  } else if (activation_function == \"relu\") {\n    H <- pmax(0, X %*% W + b)\n  }\n  # Calculate the output weights\n  beta <- solve(H) %*% y\n  # Return the ELM model\n  return(list(W = W, b = b, beta = beta, activation_function = activation_function))\n}\n\n# Define the function to predict the output\npredict <- function(model, X) {\n  if (model$activation_function == \"sigmoid\") {\n    H <- 1 / (1 + exp(-X %*% model$W - model$b))\n  } else if (model$activation_function == \"relu\") {\n    H <- pmax(0, X %*% model$W + model$b)\n  }\n  y_pred <- H %*% model$beta\n  return(y_pred)\n}\n\n# Generate some example data\nset.seed(123)\nX <- matrix(runif(100 * 5), ncol = 5)\ny <- rnorm(100)\n\n# Apply the ELM function to the data\nhidden_size <- 100\nactivation_function <- \"sigmoid\"\nmodel <- ELM(X, y, hidden_size, activation_function)\n\n# Predict the output\ny_pred <- predict(model, X)\n\n\n\n\n elm <- function(X, y, n_hidden=NULL, active_fun=tanh) {\n  # X: an N observations x p features matrix\n  # y: the target\n  # n_hidden: the number of hidden nodes\n  # active_fun: activation function\n  pp1 = ncol(X) + 1\n  w0 = matrix(rnorm(pp1*n_hidden), pp1, n_hidden)       # random weights\n  h = active_fun(cbind(1, scale(X)) %*% w0)             # compute hidden layer\n  B = MASS::ginv(h) %*% y                               # find weights for hidden layer\n  fit = h %*% B                                         # fitted values\n  list(fit= fit, loss=crossprod(fit - y), B=B, w0=w0)\n}\n\n\n\n# one variable, complex function -------------------------------------------\nlibrary(tidyverse); library(mgcv)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLoading required package: nlme\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n\nset.seed(123)\nn = 5000\nx = runif(n)\n# x = rnorm(n)\nmu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))\ny = rnorm(n, mu, .3)\n# qplot(x, y)\nd = data.frame(x,y) \n\nX_ = as.matrix(x, ncol=1)\n\ntest = elm(X_, y, n_hidden=100)\nstr(test)\n\nList of 4\n $ fit : num [1:5000, 1] -1.0239 0.7311 -0.413 0.0806 -0.4112 ...\n $ loss: num [1, 1] 442\n $ B   : num [1:100, 1] 217 -608 1408 -1433 -4575 ...\n $ w0  : num [1:2, 1:100] 0.35 0.814 -0.517 -2.692 -1.097 ...\n\n# qplot(x, y) + geom_line(aes(y=test$fit), color='#1e90ff')\ncor(test$fit[,1], y)^2\n\n[1] 0.8862518\n\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n\n[1] 0.8482127\n\nd %>% \n  mutate(fit_elm = test$fit,\n         fit_gam = fitted(gam_comparison)) %>% \n  ggplot() + \n  geom_point(aes(x, y), alpha=.1) +\n  geom_line(aes(x, y=fit_elm), color='#1e90ff') + \n  geom_line(aes(x, y=fit_gam), color='darkred')"
  },
  {
    "objectID": "posts/lasso/index.html",
    "href": "posts/lasso/index.html",
    "title": "Lasso Regression",
    "section": "",
    "text": "In this example, we first generate some sample data with n observations and p predictor variables. We then define the lambda value for the lasso regression. The predictors are standardized using the scale function.\nNext, we define the lasso_coef function to find the lasso regression coefficients. This function takes as input the predictor matrix X, the response vector y, the lambda value, and optional inputs for the tolerance (tol) and maximum number of iterations (max.iter).\nThe lasso regression coefficients are estimated using a coordinate descent algorithm. The algorithm updates each coefficient beta[j] one at a time, while holding all other coefficients fixed. The sign and pmax functions are used to perform the soft-thresholding step in the lasso penalty.\nWe then use the lasso_coef function to find the optimal values of beta that minimize the lasso regression objective. The resulting coefficients are stored in the lasso_coef variable.\nFinally, we print the estimated coefficients to the console.\n\n# Generate sample data\nset.seed(123)\nn <- 100\np <- 10\nX <- matrix(rnorm(n*p), nrow=n)\ny <- rnorm(n)\n\n# Define lambda value\nlambda <- 0.5\n\n# Standardize the predictors\nX <- scale(X)\n\n# Define lasso regression function\nlasso_coef <- function(X, y, lambda, tol=1e-6, max.iter=1000) {\n  n <- nrow(X)\n  p <- ncol(X)\n  beta <- numeric(p)\n  t <- 1\n  iter <- 1\n  while(iter < max.iter) {\n    beta.old <- beta\n    for(j in 1:p) {\n      Xj <- X[,j]\n      beta.j <- beta[-j]\n      r <- y - X[, -j] %*% beta.j\n      z <- Xj %*% r / n\n      beta[j] <- sign(z) * pmax(abs(z) - lambda, 0)\n    }\n    t <- sqrt(sum((beta - beta.old)^2))\n    if(t < tol) break\n    iter <- iter + 1\n  }\n  return(beta)\n}\n\n# Find lasso coefficients\nlasso_coef <- lasso_coef(X, y, lambda)\n\n# Print coefficients\nlasso_coef\n\n [1] 0 0 0 0 0 0 0 0 0 0"
  },
  {
    "objectID": "posts/lasso/index.html#r-code-to-estimated-a-lasso-regression-by-maximum-likelihood",
    "href": "posts/lasso/index.html#r-code-to-estimated-a-lasso-regression-by-maximum-likelihood",
    "title": "Lasso Regression",
    "section": "R code to estimated a lasso regression by maximum likelihood",
    "text": "R code to estimated a lasso regression by maximum likelihood\nIn this example, we first generate some sample data with n observations and p predictor variables. We then define the lambda value for the lasso regression. The predictors are standardized using the scale function.\nNext, we define the lasso_fun function to optimize. This function takes as input the beta vector of regression coefficients, the predictor matrix X, the response vector y, and the lambda value. It returns the sum of the residual sum of squares and the lasso penalty term.\nWe then use the optim function to find the optimal value of beta that minimizes the lasso_fun function. The starting values for beta are set to zero with rep(0,p). The other inputs to optim are the lasso_fun function, X, y, lambda, and the optimization method \"L-BFGS-B\". The par element of the output from optim contains the estimated coefficients.\nFinally, we print the estimated coefficients to the console.\n\n# Generate sample data\nset.seed(123)\nn <- 100\np <- 10\nX <- matrix(rnorm(n*p), nrow=n)\ny <- rnorm(n)\n\n# Define lambda value\nlambda <- 0.5\n\n# Standardize the predictors\nX <- scale(X)\n\n# Define function to optimize\nlasso_fun <- function(beta, X, y, lambda) {\n  sum((y - X %*% beta)^2) + lambda * sum(abs(beta))\n}\n\n# Use optim to find the lasso coefficients\nlasso_coef <- optim(par=rep(0,p), fn=lasso_fun, X=X, y=y, lambda=lambda, method=\"L-BFGS-B\")$par\n\n# Print coefficients\nlasso_coef\n\n [1]  0.056535065 -0.002470851 -0.137985850  0.142001759  0.065821245\n [6] -0.032359319  0.093055429  0.108465377 -0.050378960  0.137882801"
  },
  {
    "objectID": "posts/ridge /index.html",
    "href": "posts/ridge /index.html",
    "title": "Ridge Regression",
    "section": "",
    "text": "In this example, we first generate some sample data with n observations and p predictor variables. We then define the lambda value for the ridge regression.\nThe solve() function is used to solve the normal equation for ridge regression. We add lambda * diag(p) to the t(X) %*% X term, which is equivalent to adding the ridge penalty term to the least squares regression.\nFinally, we multiply the result by t(X) %*% y to get the ridge regression coefficients. The resulting coefficients are stored in the ridge_coef variable, which we print to the console.\n\n# Generate sample data\nset.seed(123)\nn <- 100\np <- 10\nX <- matrix(rnorm(n*p), nrow=n)\ny <- rnorm(n)\n\n# Define lambda value\nlambda <- 0.5\n\n# Calculate ridge regression coefficients\nridge_coef <- solve(t(X) %*% X + lambda * diag(p)) %*% t(X) %*% y\n\n# Print coefficients\nridge_coef\n\n             [,1]\n [1,]  0.07984794\n [2,] -0.01184439\n [3,] -0.12429433\n [4,]  0.13299966\n [5,]  0.09073672\n [6,] -0.05055385\n [7,]  0.07607811\n [8,]  0.12667890\n [9,] -0.03903223\n[10,]  0.13433382"
  },
  {
    "objectID": "posts/ridge /index.html#running-code",
    "href": "posts/ridge /index.html#running-code",
    "title": "Ridge Regression",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/ridge /index.html#r-code-to-estimated-a-ridge-regression-using-maximum-likelihood",
    "href": "posts/ridge /index.html#r-code-to-estimated-a-ridge-regression-using-maximum-likelihood",
    "title": "Ridge Regression",
    "section": "R code to estimated a Ridge regression using Maximum Likelihood",
    "text": "R code to estimated a Ridge regression using Maximum Likelihood\nIn this example, we first generate some sample data with n observations and p predictor variables. We then define the lambda value for the ridge regression.\nNext, we define the ridge_fun function to optimize. This function takes as input the beta vector of regression coefficients, the predictor matrix X, the response vector y, and the lambda value. It returns the sum of the residual sum of squares and the ridge penalty term.\nWe then use the optim function to find the optimal value of beta that minimizes the ridge_fun function. The starting values for beta are set to zero with rep(0,p). The other inputs to optim are the ridge_fun function, X, y, and lambda. The par element of the output from optim contains the estimated coefficients.\nFinally, we print the estimated coefficients to the console.\n\n# Generate sample data\nset.seed(123)\nn <- 100\np <- 10\nX <- matrix(rnorm(n*p), nrow=n)\ny <- rnorm(n)\n\n# Define lambda value\nlambda <- 0.5\n\n# Define function to optimize\nridge_fun <- function(beta, X, y, lambda) {\n  sum((y - X %*% beta)^2) + lambda * sum(beta^2)\n}\n\n# Use optim to find the ridge regression coefficients\nridge_coef <- optim(par=rep(0,p), fn=ridge_fun, X=X, y=y, lambda=lambda)$par\n\n# Print coefficients\nridge_coef\n\n [1]  0.06569894 -0.03125730 -0.14052688  0.11597308  0.05311027 -0.04749896\n [7]  0.06472437  0.10817340 -0.05156010  0.12377936"
  },
  {
    "objectID": "posts/NM/index.html",
    "href": "posts/NM/index.html",
    "title": "Nelder Mead algorithm",
    "section": "",
    "text": "nelder_mead = function(\n  f, \n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha    = 1,\n  gamma    = 2,\n  rho      = 0.5,\n  sigma    = 0.5,\n  verbose  = FALSE\n  ) {\n  # init\n  dim = length(x_start)\n  prev_best = f(x_start)\n  no_improv = 0\n  res = list(list(x_start = x_start, prev_best = prev_best))\n  \n  \n  for (i in 1:dim) {\n    x = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res = append(res, list(list(x_start = x, prev_best = score)))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    idx  = sapply(res, `[[`, 2)\n    res  = res[order(idx)]   # ascending order\n    best = res[[1]][[2]]\n    \n    # break after max_iter\n    if (max_iter > 0 & iters >= max_iter) return(res[[1]])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break) return(res[[1]])\n    \n    # centroid\n    x0 = rep(0, dim)\n    for (tup in 1:(length(res)-1)) {\n      for (i in 1:dim) {\n        x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1)\n      }\n    }\n    \n   # reflection\n   xr = x0 + alpha * (x0 - res[[length(res)]][[1]])\n   rscore = f(xr)\n   if (res[[1]][[2]] <= rscore & \n       rscore < res[[length(res)-1]][[2]]) {\n     res[[length(res)]] = list(xr, rscore)\n     next\n   }\n     \n   # expansion\n   if (rscore < res[[1]][[2]]) {\n     # xe = x0 + gamma*(x0 - res[[length(res)]][[1]])   # issue with this\n     xe = x0 + gamma * (xr - x0)   \n     escore = f(xe)\n     if (escore < rscore) {\n       res[[length(res)]] = list(xe, escore)\n       next\n     } else {\n       res[[length(res)]] = list(xr, rscore)\n       next\n     }\n   }\n   \n   # contraction\n   # xc = x0 + rho*(x0 - res[[length(res)]][[1]])  # issue with wiki consistency for rho values (and optim)\n   xc = x0 + rho * (res[[length(res)]][[1]] - x0)\n   cscore = f(xc)\n   if (cscore < res[[length(res)]][[2]]) {\n     res[[length(res)]] = list(xc, cscore)\n     next\n   }\n   \n   # reduction\n   x1   = res[[1]][[1]]\n   nres = list()\n   for (tup in res) {\n     redx  = x1 + sigma * (tup[[1]] - x1)\n     score = f(redx)\n     nres  = append(nres, list(list(redx, score)))\n   }\n   \n   res = nres\n  }\n}\n\n\n\n\n#' ## Example\n#' The function to minimize.\n#' \nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\n[[1]]\n[1] -1.570797e+00 -2.235577e-07  1.637460e-14\n\n[[2]]\n[1] -1\n\n#' Compare to `optimx`.  You may see warnings.\noptimx::optimx(\n  par = c(0, 0, 0),\n  fn = f,\n  method = \"Nelder-Mead\",\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta = 0.5,\n    maxit = 1000,\n    reltol = 1e-12\n  )\n)\n\nWarning in max(logpar): no non-missing arguments to max; returning -Inf\n\n\nWarning in min(logpar): no non-missing arguments to min; returning Inf\n\n\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n\n#' ## A Regression Model\n#' \n#' I find a regression model to be more applicable/intuitive for my needs, so\n#' provide an example for that case.\n#' \n#' \n#' ### Data setup\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n\n#' Least squares loss function.\n\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n# lm estimates\nlm.fit(X, y)$coef\n\n         x1          x2          x3          x4          x5          x6 \n-0.96214657  0.59432481  0.04864576  0.27573466  0.97525840 -0.07470287 \n\nnm_result = nelder_mead(\n  f, \n  runif(ncol(X)), \n  max_iter = 2000,\n  no_improve_thr = 1e-12,\n  verbose = FALSE\n)\n\n#' ### Comparison\n#' Compare to `optimx`.\n\nopt_out = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,  # model function\n  method  = 'Nelder-Mead',\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta  = 0.5,\n    #rho\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)\n\nrbind(\n  nm_func = unlist(nm_result),\n  nm_optimx = opt_out[1:7]\n)\n\n                  p1       p2         p3        p4        p5          p6\nnm_func   -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389\nnm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054\n             value\nnm_func   501.3155\nnm_optimx 501.3155\n\n#' # Second version\n#' \n#' This is a more natural R approach in my opinion.\n\nnelder_mead2 = function(\n  f,\n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha = 1,\n  gamma = 2,\n  rho   = 0.5,\n  sigma = 0.5,\n  verbose = FALSE\n) {\n  \n  # init\n  npar = length(x_start)\n  nc = npar + 1\n  prev_best = f(x_start)\n  no_improv = 0\n  res = matrix(c(x_start, prev_best), ncol = nc)\n  colnames(res) = c(paste('par', 1:npar, sep = '_'), 'score')\n  \n  for (i in 1:npar) {\n    x     = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res   = rbind(res, c(x, score))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    res  = res[order(res[, nc]), ]   # ascending order\n    best = res[1, nc]\n    \n    # break after max_iter\n    if (max_iter & iters >= max_iter) return(res[1, ])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break)\n      return(res[1, ])\n    \n    nr = nrow(res)\n    \n    # centroid: more efficient than previous double loop\n    x0 = colMeans(res[(1:npar), -nc])\n    \n    # reflection\n    xr = x0 + alpha * (x0 - res[nr, -nc])\n    \n    rscore = f(xr)\n    \n    if (res[1, 'score'] <= rscore & rscore < res[npar, 'score']) {\n      res[nr,] = c(xr, rscore)\n      next\n    }\n    \n    # expansion\n    if (rscore < res[1, 'score']) {\n      xe = x0 + gamma * (xr - x0)\n      escore = f(xe)\n      if (escore < rscore) {\n        res[nr, ] = c(xe, escore)\n        next\n      } else {\n        res[nr, ] = c(xr, rscore)\n        next\n      }\n    }\n    \n    # contraction\n    xc = x0 + rho * (res[nr, -nc] - x0)\n    \n    cscore = f(xc)\n    \n    if (cscore < res[nr, 'score']) {\n      res[nr,] = c(xc, cscore)\n      next\n    }\n    \n    # reduction\n    x1 = res[1, -nc]\n    \n    nres = res\n    \n    for (i in 1:nr) {\n      redx  = x1 + sigma * (res[i, -nc] - x1)\n      score = f(redx)\n      nres[i, ] = c(redx, score)\n    }\n    \n    res = nres\n  }\n}\n\n\n#' ## Example function\n\nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead2(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\n        par_1         par_2         par_3         score \n-1.570797e+00 -2.235577e-07  1.599631e-14 -1.000000e+00 \n\noptimx::optimx(\n  par = c(0, 0, 0), \n  fn = f, \n  method   = \"Nelder-Mead\",\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 1000,\n    reltol = 1e-12\n  )\n)\n\nWarning in max(logpar): no non-missing arguments to max; returning -Inf\n\nWarning in max(logpar): no non-missing arguments to min; returning Inf\n\n\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n\n#' ## A Regression Model\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n# least squares loss\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n\nlm_par = lm.fit(X, y)$coef\n\nnm_par = nelder_mead2(\n  f, \n  runif(ncol(X)),\n  max_iter = 2000,\n  no_improve_thr = 1e-12\n)\n\n\n#' ## Comparison\n#' Compare to `optimx`.\n\nopt_par = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,\n  method   = 'Nelder-Mead',\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)[1:(npreds + 1)]\n\nrbind(\n  lm = lm_par,\n  nm = nm_par,\n  optimx = opt_par,\n  truth  = beta\n)\n\nWarning in rbind(deparse.level, ...): number of columns of result, 6, is not a\nmultiple of vector length 7 of arg 2\n\n\n               p1        p2         p3        p4        p5          p6\nlm     -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287\nnm     -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389\noptimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054\ntruth  -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885"
  },
  {
    "objectID": "posts/GD/index.html",
    "href": "posts/GD/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "#' # Data Setup\n#' \n#' Create some basic data for standard regression.\n\nset.seed(8675309)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)  # model matrix\n\n\n\n#' # Gradient Descent Algorithm\n\n\ngd = function(\n  par,\n  X,\n  y,\n  tolerance = 1e-3,\n  maxit     = 1000,\n  stepsize  = 1e-3,\n  adapt     = FALSE,\n  verbose   = TRUE,\n  plotLoss  = TRUE\n  ) {\n  \n  # initialize\n  beta = par; names(beta) = colnames(X)\n  loss = crossprod(X %*% beta - y)\n  tol  = 1\n  iter = 1\n  \n  while(tol > tolerance && iter < maxit){\n    \n    LP   = X %*% beta\n    grad = t(X) %*% (LP - y)\n    betaCurrent = beta - stepsize * grad\n    tol  = max(abs(betaCurrent - beta))\n    beta = betaCurrent\n    loss = append(loss, crossprod(LP - y))\n    iter = iter + 1\n    \n    if (adapt)\n      stepsize = ifelse(\n        loss[iter] < loss[iter - 1],  \n        stepsize * 1.2, \n        stepsize * .8\n      )\n    \n    if (verbose && iter %% 10 == 0)\n      message(paste('Iteration:', iter))\n  }\n  \n  if (plotLoss)\n    plot(loss, type = 'l', bty = 'n')\n  \n  list(\n    par    = beta,\n    loss   = loss,\n    RSE    = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), \n    iter   = iter,\n    fitted = LP\n  )\n}\n\n\n#' ## Run\n#' \n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you'd have to fiddle with the `stepsize`, which could \n#' be assessed via cross-validation, or alternatively one can use an\n#' adaptive approach, a simple one of which is implemented in this function.\n\ngd_result = gd(\n  init,\n  X = X,\n  y = y,\n  tolerance = 1e-8,\n  stepsize  = 1e-4,\n  adapt     = TRUE\n)\n\nIteration: 10\n\n\nIteration: 20\n\n\nIteration: 30\n\n\nIteration: 40\n\n\nIteration: 50\n\n\nIteration: 60\n\n\nIteration: 70\n\n\n\n\nstr(gd_result)\n\nList of 5\n $ par   : num [1:3, 1] 0.985 0.487 0.218\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ loss  : num [1:70] 2315 2315 2075 1918 1760 ...\n $ RSE   : num [1, 1] 1.03\n $ iter  : num 70\n $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ...\n\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n\nrbind(\n  gd = round(gd_result$par[, 1], 5),\n  lm = coef(lm(y ~ x1 + x2))\n)\n\n   Intercept        x1        x2\ngd 0.9847800 0.4867900 0.2175200\nlm 0.9847803 0.4867896 0.2175169"
  },
  {
    "objectID": "posts/SGD/index.html",
    "href": "posts/SGD/index.html",
    "title": "Stochastic Gradient Descent",
    "section": "",
    "text": "set.seed(1234)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)\n\n\n\n#' # Stochastic Gradient Descent Algorithm\n\n\nsgd = function(\n  par,                                      # parameter estimates\n  X,                                        # model matrix\n  y,                                        # target variable\n  stepsize = 1,                             # the learning rate\n  stepsizeTau = 0,                          # if > 0, a check on the LR at early iterations\n  average = FALSE\n){\n  \n  # initialize\n  beta = par\n  names(beta) = colnames(X)\n  betamat = matrix(0, nrow(X), ncol = length(beta))      # Collect all estimates\n  fits = NA                                              # fitted values\n  s = 0                                                  # adagrad per parameter learning rate adjustment\n  loss = NA                                              # Collect loss at each point\n  \n  for (i in 1:nrow(X)) {\n    Xi   = X[i, , drop = FALSE]\n    yi   = y[i]\n    LP   = Xi %*% beta                                   # matrix operations not necessary, \n    grad = t(Xi) %*% (LP - yi)                           # but makes consistent with the  standard gd R file\n    s    = s + grad^2\n    beta = beta - stepsize * grad/(stepsizeTau + sqrt(s))     # adagrad approach\n    \n    if (average & i > 1) {\n      beta =  beta - 1/i * (betamat[i - 1, ] - beta)          # a variation\n    } \n    \n    betamat[i,] = beta\n    fits[i]     = LP\n    loss[i]     = (LP - yi)^2\n  }\n  \n  LP = X %*% beta\n  lastloss = crossprod(LP - y)\n  \n  list(\n    par    = beta,                                       # final estimates\n    parvec = betamat,                                    # all estimates\n    loss   = loss,                                       # observation level loss\n    RMSE   = sqrt(sum(lastloss)/nrow(X)),\n    fitted = fits\n  )\n}\n\n\n\n#' # Run\n\n\n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you might have to fiddle with the `stepsize`, perhaps\n#' choosing one based on cross-validation with old data.\n\nsgd_result = sgd(\n  init,\n  X = X,\n  y = y,\n  stepsize = .1,\n  stepsizeTau = .5,\n  average = FALSE\n)\n\nstr(sgd_result)\n\nList of 5\n $ par   : num [1:3, 1] 1.024 0.537 0.148\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ parvec: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ...\n $ loss  : num [1:1000] 0.67 1.261 1.365 2.043 0.215 ...\n $ RMSE  : num 1.01\n $ fitted: num [1:1000] 0 -0.0236 -0.0446 -0.2828 0.1634 ...\n\nsgd_result$par\n\n               [,1]\nIntercept 1.0241049\nx1        0.5368198\nx2        0.1478470\n\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n#' \n# summary(lm(y ~ x1 + x2))\ncoef1 = coef(lm(y ~ x1 + x2))\n\nrbind(\n  sgd_result = sgd_result$par[, 1],\n  lm = coef1\n)\n\n           Intercept        x1        x2\nsgd_result  1.024105 0.5368198 0.1478470\nlm          1.029957 0.5177020 0.1631026\n\n#' ## Visualize Estimates\n#' \nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ngd = data.frame(sgd_result$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>%\n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter',\n               values_to = 'Value') %>%\n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\nggplot(aes(\n  x = Iteration,\n  y = Value,\n  group = Parameter,\n  color = Parameter\n),\ndata = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration == n), size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    size  = 4,\n    data  = filter(gd, Iteration == n)\n  ) +\n  theme_minimal()\n\n\n\n#' # Add alternately data shift\n\n#' This data includes a shift of the previous data.\n\nset.seed(1234)\n\nn2   = 1000\nx1.2 = rnorm(n2)\nx2.2 = rnorm(n2)\ny2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2)\nX2 = rbind(X, cbind(1, x1.2, x2.2))\ncoef2 = coef(lm(y2 ~ x1.2 + x2.2))\ny2 = c(y, y2)\n\nn3    = 1000\nx1.3  = rnorm(n3)\nx2.3  = rnorm(n3)\ny3    = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3)\ncoef3 = coef(lm(y3 ~ x1.3 + x2.3))\n\nX3 = rbind(X2, cbind(1, x1.3, x2.3))\ny3 = c(y2, y3)\n\n\n\n#' ## Run\n\n\nsgd_result2 = sgd(\n  init,\n  X = X3,\n  y = y3,\n  stepsize = 1,\n  stepsizeTau = 0,\n  average = FALSE\n)\n\nstr(sgd_result2)\n\nList of 5\n $ par   : num [1:3, 1] 0.821 -0.223 0.211\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ parvec: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ...\n $ loss  : num [1:3000] 0.67 2.31 3.69 30.99 10.58 ...\n $ RMSE  : num 1.57\n $ fitted: num [1:3000] 0 -0.421 -0.797 -4.421 2.952 ...\n\n#' Compare with `lm` for each data part.\n#' \nsgd_result2$parvec[c(n, n + n2, n + n2 + n3), ]\n\n           [,1]       [,2]       [,3]\n[1,]  1.0859378  0.5128904  0.1457697\n[2,] -0.9246994  0.2945723 -0.2941759\n[3,]  0.8213521 -0.2229918  0.2112883\n\nrbind(coef1, coef2, coef3)\n\n      (Intercept)         x1         x2\ncoef1   1.0299573  0.5177020  0.1631026\ncoef2  -0.9700427  0.2677020 -0.2868974\ncoef3   1.0453166 -0.2358521  0.2418489\n\n#' Visualize estimates.\n#' \ngd = data.frame(sgd_result2$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>% \n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter', \n               values_to = 'Value') %>% \n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\n\nggplot(aes(x = Iteration,\n           y = Value,\n           group = Parameter,\n           color = Parameter\n           ),\n       data = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n             size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n    size = 4,\n    show.legend = FALSE\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/NN/index.html",
    "href": "posts/NN/index.html",
    "title": "Neural Network",
    "section": "",
    "text": "X = matrix( \n  c(0, 0, 1, \n    0, 1, 1, \n    1, 0, 1, \n    1, 1, 1),\n  nrow  = 4,\n  ncol  = 3,\n  byrow = TRUE\n)\n\n# output dataset            \ny = c(0, 0, 1, 1)\n\n# seed random numbers to make calculation\n# deterministic (just a good practice)\nset.seed(1)\n\n# initialize weights randomly with mean 0\nsynapse_0 = matrix(runif(3, min = -1, max = 1), 3, 1)\n\n# sigmoid function\nnonlin <- function(x, deriv = FALSE) {\n  if (deriv)\n    x * (1 - x)\n  else\n    plogis(x)\n}\n\n\nnn_1 <- function(X, y, synapse_0, maxiter = 10000) {\n  \n  for (iter in 1:maxiter) {\n  \n      # forward propagation\n      layer_0 = X\n      layer_1 = nonlin(layer_0 %*% synapse_0)\n  \n      # how much did we miss?\n      layer_1_error = y - layer_1\n  \n      # multiply how much we missed by the \n      # slope of the sigmoid at the values in layer_1\n      l1_delta = layer_1_error * nonlin(layer_1, deriv = TRUE)\n  \n      # update weights\n      synapse_0 = synapse_0 + crossprod(layer_0, l1_delta)\n  }\n  \n  list(layer_1 = layer_1, layer_1_error = layer_1_error, synapse_0 = synapse_0)\n}\n\nfit_nn = nn_1(X, y, synapse_0)\n\nmessage(\"Output After Training: \\n\", \n        paste0(capture.output(cbind(fit_nn$layer_1, y)), collapse = '\\n'))\n\nOutput After Training: \n                 y\n[1,] 0.009670417 0\n[2,] 0.007864211 0\n[3,] 0.993590571 1\n[4,] 0.992115835 1\n\n\n\nX = matrix(\n  c(0, 0, 1,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 1),\n  nrow = 4,\n  ncol = 3,\n  byrow = TRUE\n)\n\ny = matrix(as.integer(xor(X[,1], X[,2])), ncol = 1)  # make the relationship explicit\n\nset.seed(1)\n\n# or do randomly in same fashion\nsynapse_0 = matrix(runif(12, -1, 1), 3, 4)\nsynapse_1 = matrix(runif(12, -1, 1), 4, 1)\n\nWarning in matrix(runif(12, -1, 1), 4, 1): data length differs from size of\nmatrix: [12 != 4 x 1]\n\n# synapse_0\n# synapse_1\n\nnn_2 <- function(\n  X,\n  y,\n  synapse_0_start,\n  synapse_1_start,\n  maxiter = 30000,\n  verbose = TRUE\n) {\n    \n  synapse_0 = synapse_0_start\n  synapse_1 = synapse_1_start\n  \n  for (j in 1:maxiter) {\n    layer_1 = plogis(X  %*% synapse_0)              # 4 x 4\n    layer_2 = plogis(layer_1 %*% synapse_1)         # 4 x 1\n    \n    # how much did we miss the target value?\n    layer_2_error = y - layer_2\n    \n    if (verbose && (j %% 10000) == 0) {\n      message(glue::glue(\"Error: {mean(abs(layer_2_error))}\"))\n    }\n  \n    # in what direction is the target value?\n    # were we really sure? if so, don't change too much.\n    layer_2_delta = (y - layer_2) * (layer_2 * (1 - layer_2))\n    \n    # how much did each l1 value contribute to the l2 error (according to the weights)?\n    layer_1_error = layer_2_delta %*% t(synapse_1)\n    \n    # in what direction is the target l1?\n    # were we really sure? if so, don't change too much.  \n    layer_1_delta = tcrossprod(layer_2_delta, synapse_1) * (layer_1 * (1 - layer_1))\n    \n    # update\n    synapse_1 = synapse_1 + crossprod(layer_1, layer_2_delta)\n    synapse_0 = synapse_0 + crossprod(X, layer_1_delta)\n  }\n  \n  list(\n    layer_1_error = layer_1_error,\n    layer_2_error = layer_2_error,\n    synapse_0 = synapse_0,\n    synapse_1 = synapse_1,\n    layer_1 = layer_1,\n    layer_2 = layer_2\n  )\n}\n\n\nfit_nn = nn_2(\n  X,\n  y,\n  synapse_0_start = synapse_0,\n  synapse_1_start = synapse_1,\n  maxiter = 30000\n)\n\nError: 0.0105538166393651\n\n\nError: 0.00729252475321202\n\n\nError: 0.0058973637409426\n\n\n\nglue::glue('Final error: {round(mean(abs(fit_nn$layer_2_error)), 5)}')\n\nFinal error: 0.0059\n\n\n\nround(fit_nn$layer_1, 3)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] 0.259 0.889 0.364 0.445\n[2,] 0.000 0.037 0.978 0.030\n[3,] 0.946 1.000 0.984 0.020\n[4,] 0.016 0.984 1.000 0.001\n\n\n\nround(cbind(fit_nn$layer_2, y), 3)\n\n      [,1] [,2]\n[1,] 0.002    0\n[2,] 0.993    1\n[3,] 0.994    1\n[4,] 0.008    0\n\n\n\nround(fit_nn$synapse_0, 3)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,]  3.915  7.364  4.705 -3.669\n[2,] -6.970 -5.351  4.337 -3.242\n[3,] -1.050  2.079 -0.559 -0.221\n\n\n\nround(fit_nn$synapse_1, 3)\n\n        [,1]\n[1,]  10.988\n[2,] -10.733\n[3,]   5.576\n[4,]  -2.987"
  },
  {
    "objectID": "posts/CSM/index.html",
    "href": "posts/CSM/index.html",
    "title": "Cubic Spline Model",
    "section": "",
    "text": "library(tidyverse) # for processing and plotting\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n#' # Create the data\nsize = c(1.42,1.58,1.78,1.99,1.99,1.99,2.13,2.13,2.13,\n         2.32,2.32,2.32,2.32,2.32,2.43,2.43,2.78,2.98,2.98)\n\nwear = c(4.0,4.2,2.5,2.6,2.8,2.4,3.2,2.4,2.6,4.8,2.9,\n         3.8,3.0,2.7,3.1,3.3,3.0,2.8,1.7)\n\nx = size - min(size)\nx = x / max(x)\nd = data.frame(wear, x)\n\n#' Cubic spline function\nrk <- function(x, z) {\n  ((z-0.5)^2 - 1/12) * ((x-0.5)^2 - 1/12)/4 -\n    ((abs(x-z)-0.5)^4 - (abs(x-z)-0.5)^2/2 + 7/240) / 24\n}\n\n#' Generate the model matrix.\nsplX <- function(x, knots) {\n  q = length(knots) + 2                # number of parameters\n  n = length(x)                        # number of observations\n  X = matrix(1, n, q)                  # initialized model matrix\n  X[ ,2]   = x                         # set second column to x\n  X[ ,3:q] = outer(x, knots, FUN = rk) # remaining to cubic spline basis\n  X\n}\n\nsplS <- function(knots) {\n  q = length(knots) + 2\n  S = matrix(0, q, q)                         # initialize matrix\n  S[3:q, 3:q] = outer(knots, knots, FUN = rk) # fill in non-zero part\n  S\n}\n\n#' Matrix square root function. Note that there are various packages with their own.\nmatSqrt <- function(S) {\n  d  = eigen(S, symmetric = T)\n  rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors)\n  rS\n}\n\n#' Penalized fitting function.\nprsFit <- function(y, x, knots, lambda) {\n  q  = length(knots) + 2    # dimension of basis\n  n  = length(x)            # number of observations\n  Xa = rbind(splX(x, knots), matSqrt(splS(knots))*sqrt(lambda)) # augmented model matrix\n  y[(n+1):(n+q)] = 0        # augment the data vector\n  \n  lm(y ~ Xa - 1) # fit and return penalized regression spline\n}\n\n\n\n#' # Example 1\n\n\n#' Unpenalized\n#' \nknots = 1:4/5\nX = splX(x, knots)      # generate model matrix\nmod1 = lm(wear ~ X - 1) # fit model\n\nxp = 0:100/100 # x values for prediction\nXp = splX(xp, knots) # prediction matrix\n\n\n#' Visualize\n\nggplot(aes(x = x, y = wear), data = data.frame(x, wear)) +\n  geom_point(color = \"#FF5500\") +\n  geom_line(aes(x = xp, y = Xp %*% coef(mod1)),\n            data = data.frame(xp, Xp),\n            color = \"#00AAFF\") +\n  labs(x = 'Scaled Engine size', y  = 'Wear Index') +\n  theme_minimal()\n\n\n\n#' # Example 2\n\n\n# Add penalty lambda\nknots = 1:7/8\nd2 = data.frame(x = xp)\n\nfor (i in c(.1, .01, .001, .0001, .00001, .000001)){\n  # fit penalized regression\n  mod2 = prsFit(\n    y = wear,\n    x = x,\n    knots = knots,\n    lambda = i\n  ) \n  # spline choosing lambda\n  Xp = splX(xp, knots) # matrix to map parameters to fitted values at xp\n  LP = Xp %*% coef(mod2)\n  d2[, paste0('lambda = ', i)] = LP[, 1]\n}\n\n#' Examine\n# head(d2)\n\n#' Visualize via ggplot\nd3 = d2 %>%\n  pivot_longer(cols = -x,\n               names_to  = 'lambda',\n               values_to = 'value') %>% \n  mutate(lambda = fct_inorder(lambda))\n\nggplot(d3) +\n  geom_point(aes(x = x, y = wear), col = '#FF5500', data = d) +\n  geom_line(aes(x = x, y = value), col = \"#00AAFF\") +\n  facet_wrap(~lambda) +\n  theme_minimal()"
  }
]